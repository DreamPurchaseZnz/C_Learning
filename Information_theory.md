# [Information theory Glossary](https://en.wikipedia.org/wiki/Entropy)

>Entropy: a complete lack of order, a way of measuring the lack of order that exists in a system. In chinese, I would call it "混沌" rather
>than "熵".

When viewed in terms of information theory, the entropy state function is simply the amount of information(in shannon sense) that would be
*need* to specify the full microstate of the system

In information theory, entropy is the measure of the amount of information that is missing before reception and is sometimes referred to as
shannon entropy
